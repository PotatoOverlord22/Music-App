{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "225ab3aa",
   "metadata": {},
   "source": [
    "# Recommender system that takes user mood, time and certain audio features into consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e95199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d19fab9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU detected and enabled: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU detected and enabled:\", gpus)\n",
    "    except RuntimeError as e:\n",
    "        print(\"Error enabling GPU:\", e)\n",
    "else:\n",
    "    print(\"No GPU detected. Running on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc033dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing model from disk.\n",
      "Epoch 1/30\n",
      "770/770 [==============================] - 7s 6ms/step - loss: 1.2498 - accuracy: 0.5389 - val_loss: 1.2507 - val_accuracy: 0.5398\n",
      "Epoch 2/30\n",
      "770/770 [==============================] - 4s 6ms/step - loss: 1.2509 - accuracy: 0.5382 - val_loss: 1.2528 - val_accuracy: 0.5376\n",
      "Epoch 3/30\n",
      "770/770 [==============================] - 5s 6ms/step - loss: 1.2444 - accuracy: 0.5399 - val_loss: 1.2494 - val_accuracy: 0.5435\n",
      "Epoch 4/30\n",
      "770/770 [==============================] - 5s 6ms/step - loss: 1.2429 - accuracy: 0.5417 - val_loss: 1.2544 - val_accuracy: 0.5369\n",
      "Epoch 5/30\n",
      "770/770 [==============================] - 5s 6ms/step - loss: 1.2444 - accuracy: 0.5391 - val_loss: 1.2496 - val_accuracy: 0.5394\n",
      "Epoch 6/30\n",
      "770/770 [==============================] - 4s 6ms/step - loss: 1.2494 - accuracy: 0.5371 - val_loss: 1.2522 - val_accuracy: 0.5409\n",
      "Epoch 7/30\n",
      "770/770 [==============================] - 5s 6ms/step - loss: 1.2488 - accuracy: 0.5355 - val_loss: 1.2535 - val_accuracy: 0.5435\n",
      "Epoch 8/30\n",
      "770/770 [==============================] - 5s 6ms/step - loss: 1.2455 - accuracy: 0.5382 - val_loss: 1.2537 - val_accuracy: 0.5427\n",
      "Epoch 9/30\n",
      "770/770 [==============================] - 4s 5ms/step - loss: 1.2489 - accuracy: 0.5399 - val_loss: 1.2468 - val_accuracy: 0.5409\n",
      "Epoch 10/30\n",
      "770/770 [==============================] - 5s 6ms/step - loss: 1.2442 - accuracy: 0.5412 - val_loss: 1.2504 - val_accuracy: 0.5369\n",
      "Epoch 11/30\n",
      "770/770 [==============================] - 5s 6ms/step - loss: 1.2463 - accuracy: 0.5377 - val_loss: 1.2477 - val_accuracy: 0.5460\n",
      "Epoch 12/30\n",
      "770/770 [==============================] - 4s 6ms/step - loss: 1.2458 - accuracy: 0.5360 - val_loss: 1.2510 - val_accuracy: 0.5383\n",
      "Epoch 13/30\n",
      "770/770 [==============================] - 4s 5ms/step - loss: 1.2455 - accuracy: 0.5408 - val_loss: 1.2522 - val_accuracy: 0.5369\n",
      "Epoch 14/30\n",
      "770/770 [==============================] - 4s 6ms/step - loss: 1.2460 - accuracy: 0.5361 - val_loss: 1.2502 - val_accuracy: 0.5416\n",
      "Epoch 15/30\n",
      "770/770 [==============================] - 5s 6ms/step - loss: 1.2422 - accuracy: 0.5391 - val_loss: 1.2504 - val_accuracy: 0.5420\n",
      "Epoch 16/30\n",
      "770/770 [==============================] - 4s 6ms/step - loss: 1.2460 - accuracy: 0.5352 - val_loss: 1.2471 - val_accuracy: 0.5467\n",
      "Epoch 17/30\n",
      "770/770 [==============================] - 4s 5ms/step - loss: 1.2434 - accuracy: 0.5401 - val_loss: 1.2505 - val_accuracy: 0.5420\n",
      "Epoch 18/30\n",
      "770/770 [==============================] - 4s 6ms/step - loss: 1.2455 - accuracy: 0.5406 - val_loss: 1.2496 - val_accuracy: 0.5471\n",
      "Epoch 19/30\n",
      "770/770 [==============================] - 4s 5ms/step - loss: 1.2437 - accuracy: 0.5409 - val_loss: 1.2503 - val_accuracy: 0.5402\n",
      "Epoch 20/30\n",
      "770/770 [==============================] - 4s 5ms/step - loss: 1.2467 - accuracy: 0.5384 - val_loss: 1.2500 - val_accuracy: 0.5478\n",
      "Epoch 21/30\n",
      "770/770 [==============================] - 4s 5ms/step - loss: 1.2412 - accuracy: 0.5354 - val_loss: 1.2512 - val_accuracy: 0.5449\n",
      "Epoch 22/30\n",
      "770/770 [==============================] - 4s 5ms/step - loss: 1.2441 - accuracy: 0.5382 - val_loss: 1.2481 - val_accuracy: 0.5416\n",
      "Epoch 23/30\n",
      "770/770 [==============================] - 4s 5ms/step - loss: 1.2399 - accuracy: 0.5390 - val_loss: 1.2487 - val_accuracy: 0.5416\n",
      "Epoch 24/30\n",
      "770/770 [==============================] - 4s 6ms/step - loss: 1.2423 - accuracy: 0.5403 - val_loss: 1.2521 - val_accuracy: 0.5446\n",
      "Epoch 25/30\n",
      "770/770 [==============================] - 4s 6ms/step - loss: 1.2414 - accuracy: 0.5367 - val_loss: 1.2506 - val_accuracy: 0.5402\n",
      "Epoch 26/30\n",
      "770/770 [==============================] - 4s 6ms/step - loss: 1.2443 - accuracy: 0.5350 - val_loss: 1.2492 - val_accuracy: 0.5402\n",
      "Epoch 27/30\n",
      "770/770 [==============================] - 5s 6ms/step - loss: 1.2465 - accuracy: 0.5361 - val_loss: 1.2494 - val_accuracy: 0.5391\n",
      "Epoch 28/30\n",
      "770/770 [==============================] - 5s 7ms/step - loss: 1.2466 - accuracy: 0.5376 - val_loss: 1.2517 - val_accuracy: 0.5380\n",
      "Epoch 29/30\n",
      "770/770 [==============================] - 5s 7ms/step - loss: 1.2400 - accuracy: 0.5383 - val_loss: 1.2513 - val_accuracy: 0.5365\n",
      "Epoch 30/30\n",
      "770/770 [==============================] - 5s 6ms/step - loss: 1.2404 - accuracy: 0.5362 - val_loss: 1.2518 - val_accuracy: 0.5387\n",
      "214/214 [==============================] - 1s 4ms/step - loss: 1.2161 - accuracy: 0.5624\n",
      "Test Loss: 1.2161, Test Accuracy: 0.5624\n",
      "INFO:tensorflow:Assets written to: ./saved_models/genre_recommender\\assets\n",
      "Model saved to ./saved_models/genre_recommender\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "[('pop', 0.7248019), ('disco', 0.24400248), ('rock', 0.010168788)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venvs\\ai\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\venvs\\ai\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\venvs\\ai\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Adjust path as needed\n",
    "DATA_PATH = './Data/cars_dataset.csv'\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Quick look\n",
    "df.head()\n",
    "\n",
    "# %% [markdown]\n",
    "## 3. Preprocessing\n",
    "\n",
    "### 3.1 Encode Target (Genre)\n",
    "# One-hot encode genres\n",
    "genre_encoder = OneHotEncoder(sparse_output=False)\n",
    "genre_ohe = genre_encoder.fit_transform(df[['genre']])\n",
    "\n",
    "### 3.2 Numerical Features Scaling\n",
    "# Scale numerical audio features\n",
    "num_cols = ['popularity','release','danceability','energy','valence','tempo','duration_ms']\n",
    "scaler = StandardScaler()\n",
    "num_scaled = scaler.fit_transform(df[num_cols])\n",
    "\n",
    "### 3.3 Categorical Context (Mood & Time of Day)\n",
    "# One-hot encode user mood and time of day\n",
    "mood_encoder = OneHotEncoder(sparse_output=False)\n",
    "mood_ohe = mood_encoder.fit_transform(df[['mood']])\n",
    "\n",
    "time_encoder = OneHotEncoder(sparse_output=False)\n",
    "time_ohe = time_encoder.fit_transform(df[['time_of_day']])\n",
    "\n",
    "### 3.4 Combine Features\n",
    "# Audio and context features\n",
    "X_audio = num_scaled\n",
    "X_context = np.hstack([mood_ohe, time_ohe])\n",
    "\n",
    "# Full feature matrix and target labels\n",
    "X = np.hstack([X_audio, X_context])\n",
    "y = genre_ohe\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=df['genre']\n",
    ")\n",
    "\n",
    "model_dir = './saved_models/genre_recommender'\n",
    "if os.path.exists(model_dir):\n",
    "    model = tf.keras.models.load_model(model_dir)\n",
    "    print(\"Loaded existing model from disk.\")\n",
    "else:\n",
    "    print(\"No existing model found; training from scratch.\")\n",
    "    audio_inputs = layers.Input(shape=(X_audio.shape[1],), name='audio_input')\n",
    "    context_inputs = layers.Input(shape=(X_context.shape[1],), name='context_input')\n",
    "\n",
    "    # Audio branch\n",
    "    x1 = layers.Dense(64, activation='relu')(audio_inputs)\n",
    "    x1 = layers.Dropout(0.3)(x1)\n",
    "\n",
    "    # Context branch\n",
    "    x2 = layers.Dense(32, activation='relu')(context_inputs)\n",
    "    x2 = layers.Dropout(0.3)(x2)\n",
    "\n",
    "    # Fuse\n",
    "    x = layers.Concatenate()([x1, x2])\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(genre_ohe.shape[1], activation='softmax', name='genre_output')(x)\n",
    "    \n",
    "    # Build and compile model\n",
    "    model = models.Model(inputs=[audio_inputs, context_inputs], outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    model.summary()\n",
    "\n",
    "# %% [markdown]\n",
    "## 5. Train\n",
    "\n",
    "history = model.fit(\n",
    "    {'audio_input': X_train[:, :X_audio.shape[1]],\n",
    "     'context_input': X_train[:, X_audio.shape[1]:]},\n",
    "    y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=30,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "## 6. Evaluate\n",
    "\n",
    "eval_results = model.evaluate(\n",
    "    {'audio_input': X_test[:, :X_audio.shape[1]],\n",
    "     'context_input': X_test[:, X_audio.shape[1]:]},\n",
    "    y_test\n",
    ")\n",
    "print(f\"Test Loss: {eval_results[0]:.4f}, Test Accuracy: {eval_results[1]:.4f}\")\n",
    "\n",
    "# save model\n",
    "os.makedirs('./saved_models', exist_ok=True)\n",
    "model.save('./saved_models/genre_recommender')\n",
    "print(\"Model saved to ./saved_models/genre_recommender\")\n",
    "\n",
    "# %% [markdown]\n",
    "## 7. Inference: Recommending a Genre for New Context\n",
    "\n",
    "def recommend_genre(audio_feature_vec, user_mood, user_time_of_day, top_k=3):\n",
    "    # Encode new sample\n",
    "    audio_scaled = scaler.transform([audio_feature_vec])\n",
    "    mood_vec = mood_encoder.transform([[user_mood]])\n",
    "    time_vec = time_encoder.transform([[user_time_of_day]])\n",
    "    context_vec = np.hstack([mood_vec, time_vec])\n",
    "    \n",
    "    # Predict genre probabilities\n",
    "    probs = model.predict(\n",
    "        {'audio_input': audio_scaled,\n",
    "         'context_input': context_vec}\n",
    "    )[0]\n",
    "    top_idx = np.argsort(probs)[-top_k:][::-1]\n",
    "    return [(genre_encoder.categories_[0][i], probs[i]) for i in top_idx]\n",
    "\n",
    "# Example usage:\n",
    "sample_audio = df.loc[0, num_cols].values\n",
    "print(recommend_genre(sample_audio, user_mood='energetic', user_time_of_day='morning'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710892b3",
   "metadata": {},
   "source": [
    "# Export encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76e868d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved encoders to ./saved_models/genre_recommender/\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save encoders\n",
    "with open('./saved_models/genre_recommender/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "with open('./saved_models/genre_recommender/mood_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(mood_encoder, f)\n",
    "\n",
    "with open('./saved_models/genre_recommender/time_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(time_encoder, f)\n",
    "\n",
    "with open('./saved_models/genre_recommender/genre_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(genre_encoder, f)\n",
    "\n",
    "print(\"Saved encoders to ./saved_models/genre_recommender/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7a9e35",
   "metadata": {},
   "source": [
    "# Extracting required audio features for the recommender system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e9c3032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'danceability': 0.9776987920699817, 'energy': 0.1301843225955963, 'valence': 0.438683973870867, 'tempo': 123.046875, 'key': 7, 'mode': 1, 'duration_ms': 30013}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rauli\\AppData\\Local\\Temp\\ipykernel_20112\\2550456036.py:42: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  'tempo':       float(tempo),\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from scipy.stats import hmean\n",
    "\n",
    "def extract_audio_features(path):\n",
    "    # 1) Load audio\n",
    "    y, sr = librosa.load(path, sr=22050, mono=True)\n",
    "    duration = librosa.get_duration(y=y, sr=sr)  # in seconds\n",
    "\n",
    "    # 2) Tempo (BPM) & Beat‐based features\n",
    "    tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr)\n",
    "    # danceability ≈ normalized variance of inter‐beat interval\n",
    "    ibis = np.diff(librosa.frames_to_time(beat_frames, sr=sr))\n",
    "    danceability = 1.0 / (1.0 + np.std(ibis) / np.mean(ibis))\n",
    "\n",
    "    # 3) Energy: average root‐mean‐square energy over frames\n",
    "    rms = librosa.feature.rms(y=y)[0]\n",
    "    energy = np.mean(rms)\n",
    "\n",
    "    # 4) Valence proxy: spectral centroid & spectral contrast\n",
    "    cent = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "    contrast = librosa.feature.spectral_contrast(y=y, sr=sr)[0]\n",
    "    # combine into a “brightness” metric as a rough valence estimate\n",
    "    valence = (np.mean(cent) / np.max(cent) + np.mean(contrast) / np.max(contrast)) / 2\n",
    "\n",
    "    # 5) Key & Mode: via chroma\n",
    "    chroma = librosa.feature.chroma_cqt(y=y, sr=sr)\n",
    "    chroma_avg = np.mean(chroma, axis=1)\n",
    "    # simplest: pick the pitch class with highest energy\n",
    "    key_idx = chroma_avg.argmax()\n",
    "    # mode: compare averaged energy in minor thirds vs. major thirds\n",
    "    # (this is very heuristic—better models exist!)\n",
    "    # major third above key\n",
    "    major_third = chroma_avg[(key_idx + 4) % 12]\n",
    "    minor_third = chroma_avg[(key_idx + 3) % 12]\n",
    "    mode = 1 if major_third > minor_third else 0\n",
    "\n",
    "    return {\n",
    "        'danceability': float(danceability),\n",
    "        'energy':      float(energy),\n",
    "        'valence':     float(valence),\n",
    "        'tempo':       float(tempo),\n",
    "        'key':         int(key_idx),\n",
    "        'mode':        int(mode),\n",
    "        'duration_ms': int(duration * 1000)\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "features = extract_audio_features('./Data/genres_original/blues/blues.00000.wav')\n",
    "print(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22d2de50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available mood categories:\n",
      "['angry', 'dreamy', 'emotional', 'energetic', 'happy', 'intense', 'peaceful', 'relaxed', 'romantic', 'sad']\n",
      "Total number of mood categories: 10\n",
      "\n",
      "Available time of day categories:\n",
      "['afternoon', 'evening', 'morning', 'night']\n",
      "Total number of time of day categories: 4\n",
      "\n",
      "Model's context input shape: [(None, 14)]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis mismatch is likely causing your error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Call this function to display the information\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[43mdisplay_available_categories\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 27\u001b[0m, in \u001b[0;36mdisplay_available_categories\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m context_input \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_layer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext_input\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms context input shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_input\u001b[38;5;241m.\u001b[39minput_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected total number of context features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_input\u001b[38;5;241m.\u001b[39minput_shape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# For comparison with your current constants\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCurrent MOOD_MAP length:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(MOOD_MAP))  \u001b[38;5;66;03m# Your global constant\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def display_available_categories():\n",
    "    # Load the model first\n",
    "    model = tf.keras.models.load_model('./saved_models/genre_recommender')\n",
    "    \n",
    "    # Load your data to recreate the encoders\n",
    "    df = pd.read_csv('./Data/cars_dataset.csv')\n",
    "    \n",
    "    # Recreate encoders and fit them to get the categories\n",
    "    mood_encoder = OneHotEncoder(sparse_output=False)\n",
    "    mood_encoder.fit(df[['mood']])\n",
    "    \n",
    "    time_encoder = OneHotEncoder(sparse_output=False)\n",
    "    time_encoder.fit(df[['time_of_day']])\n",
    "    \n",
    "    # Display the categories\n",
    "    print(\"Available mood categories:\")\n",
    "    print(mood_encoder.categories_[0].tolist())\n",
    "    print(f\"Total number of mood categories: {len(mood_encoder.categories_[0])}\")\n",
    "    \n",
    "    print(\"\\nAvailable time of day categories:\")\n",
    "    print(time_encoder.categories_[0].tolist())\n",
    "    print(f\"Total number of time of day categories: {len(time_encoder.categories_[0])}\")\n",
    "    \n",
    "    # Also check the expected shape of the context input\n",
    "    context_input = model.get_layer('context_input')\n",
    "    print(f\"\\nModel's context input shape: {context_input.input_shape}\")\n",
    "    print(f\"Expected total number of context features: {context_input.input_shape[1]}\")\n",
    "    \n",
    "    # For comparison with your current constants\n",
    "    print(\"\\nCurrent MOOD_MAP length:\", len(MOOD_MAP))  # Your global constant\n",
    "    print(\"Current TIME_MAP length:\", len(TIME_MAP))    # Your global constant\n",
    "    \n",
    "    # Calculate expected vs actual context size\n",
    "    expected_context_size = context_input.input_shape[1]\n",
    "    actual_context_size = len(mood_encoder.categories_[0]) + len(time_encoder.categories_[0])\n",
    "    \n",
    "    if expected_context_size != actual_context_size:\n",
    "        print(f\"\\nWARNING: Model expects {expected_context_size} context features, but encoders produce {actual_context_size} features\")\n",
    "        print(\"This mismatch is likely causing your error\")\n",
    "\n",
    "# Call this function to display the information\n",
    "display_available_categories()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AI)",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
