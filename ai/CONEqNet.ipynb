{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define genre EQ profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefine equalization profiles for each genre.\n",
    "# Each profile is an array of 10 values (gains in decibels) corresponding to:\n",
    "# [32, 64, 125, 250, 500, 1000, 2000, 4000, 8000, 16000 Hz]\n",
    "genre_profiles = {\n",
    "    'blues':     np.array([-1,  0,  2,  2,  1,  0, -1,  0,  1,  1]),\n",
    "    'classical': np.array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
    "    'country':   np.array([ 0,  1,  1,  2,  1,  0,  0,  0,  0,  0]),\n",
    "    'disco':     np.array([ 2,  3,  2,  1,  0,  0,  1,  2,  3,  2]),\n",
    "    'hiphop':    np.array([ 3,  4,  2,  0, -1, -1,  0,  1,  1,  0]),\n",
    "    'jazz':      np.array([ 0,  1,  1,  2,  2,  1,  0,  0,  1,  0]),\n",
    "    'metal':     np.array([ 2,  3,  0, -3, -4, -3,  0,  3,  3,  2]),\n",
    "    'pop':       np.array([ 0,  1,  2,  2,  1,  0,  1,  1,  2,  2]),\n",
    "    'reggae':    np.array([ 0,  1,  0,  0, -1, -1,  0,  0,  1,  0]),\n",
    "    'rock':      np.array([ 1,  1,  0,  0,  1,  1,  0,  0,  1,  1])\n",
    "}\n",
    "\n",
    "# Ensure this order matches the model's output order.\n",
    "genres = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
    "\n",
    "def weighted_eq_profile(predictions, genre_list=genres, profiles=genre_profiles):\n",
    "    \"\"\"\n",
    "    Given a predictions vector (softmax output over genres), compute a weighted\n",
    "    equalization profile based on the top three predicted genres.\n",
    "    \n",
    "    Parameters:\n",
    "      predictions: numpy array of shape (10,) with probabilities for each genre.\n",
    "      genre_list: list of genre names in the order corresponding to predictions.\n",
    "      profiles: dictionary mapping genre names to their equalization profile (numpy array).\n",
    "      \n",
    "    Returns:\n",
    "      weighted_profile: numpy array of shape (10,) representing the weighted EQ profile.\n",
    "    \"\"\"\n",
    "    # Get indices of the top 3 predicted genres.\n",
    "    top3_idx = predictions.argsort()[-3:][::-1]\n",
    "    top3_probs = predictions[top3_idx]\n",
    "    \n",
    "    # Normalize the top 3 probabilities so they sum to 1.\n",
    "    weight_sum = np.sum(top3_probs)\n",
    "    if weight_sum == 0:\n",
    "        norm_weights = np.ones_like(top3_probs) / 3\n",
    "    else:\n",
    "        norm_weights = top3_probs / weight_sum\n",
    "    \n",
    "    # Compute the weighted combination of the equalization profiles.\n",
    "    weighted_profile = np.zeros_like(profiles[genre_list[0]], dtype=float)\n",
    "    for i, idx in enumerate(top3_idx):\n",
    "        genre = genre_list[idx]\n",
    "        profile = profiles[genre]\n",
    "        weighted_profile += norm_weights[i] * profile\n",
    "    \n",
    "    return weighted_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define or get CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path_keras = \"./saved_models/CONEqNet.keras\"\n",
    "model_save_path_h5 = \"./saved_models/CONEqNet.h5\"\n",
    "\n",
    "if os.path.exists(model_save_path_keras):\n",
    "    model = tf.keras.models.load_model(model_save_path_keras)\n",
    "    print(f\"Loaded existing model from {model_save_path_keras}\")\n",
    "elif os.path.exists(model_save_path_h5):\n",
    "    model = tf.keras.models.load_model(model_save_path_h5)\n",
    "    print(f\"Loaded existing model from {model_save_path_h5}\")\n",
    "else:\n",
    "    # 600 time steps, 13 MFCC coefficients, 1 channel (grayscale-like image)\n",
    "    print(\"No saved model found, creating a new one...\")\n",
    "    input_shape = (600, 13, 1)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Low-Level Features (Input):\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Mid-Level Features (Convolutional Layers):\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Upper-Level features (fully connected layers and output):\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    print(\"New model successfully compiled.\")\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths (note: genres are inside a subfolder per genre)\n",
    "data_dir = './Data'\n",
    "audio_dir = os.path.normpath(os.path.join(data_dir, 'genres_original'))\n",
    "csv_path = os.path.normpath(os.path.join(data_dir, 'features_30_sec.csv'))\n",
    "\n",
    "df_features = pd.read_csv(csv_path)\n",
    "\n",
    "df_features['filepath'] = df_features.apply(lambda row: os.path.join(audio_dir, row['label'], row['filename']), axis=1)\n",
    "print(\"\\nAdded filepath to dataframe features.\")\n",
    "\n",
    "genres = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
    "label_to_index = {genre: i for i, genre in enumerate(genres)}\n",
    "\n",
    "def extract_mfcc(file_path, n_mfcc=13, hop_length=1103):\n",
    "    y, sr = librosa.load(file_path, duration=30)\n",
    "        \n",
    "    # the resulting shape is (n_mfcc, n_frames)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length)\n",
    "    return mfcc\n",
    "\n",
    "# extract MFCCs for each audio file\n",
    "def prepare_data(df, target_frames=600, n_mfcc=13, hop_length=1103):\n",
    "    X = []\n",
    "    y = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            mfcc = extract_mfcc(row['filepath'], n_mfcc=n_mfcc, hop_length=hop_length)\n",
    "            mfcc = mfcc.T\n",
    "            # pad/truncate\n",
    "            if mfcc.shape[0] < target_frames:\n",
    "                pad_width = target_frames - mfcc.shape[0]\n",
    "                mfcc = np.pad(mfcc, ((0, pad_width), (0, 0)), mode='constant')\n",
    "            elif mfcc.shape[0] > target_frames:\n",
    "                mfcc = mfcc[:target_frames, :]\n",
    "                \n",
    "            mfcc = mfcc[..., np.newaxis]\n",
    "            X.append(mfcc)\n",
    "            y.append(label_to_index[row['label']])\n",
    "        except Exception as e:\n",
    "            print(\"Error processing {}: {}\".format(row['filepath'], e))\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Make tensorflow use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU detected and enabled:\", gpus)\n",
    "    except RuntimeError as e:\n",
    "        print(\"Error enabling GPU:\", e)\n",
    "else:\n",
    "    print(\"No GPU detected. Running on CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split and data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df_features, test_size=0.2, stratify=df_features['label'], random_state=42)\n",
    "print(\"Train set shape:\", train_df.shape)\n",
    "print(\"Validation set shape:\", val_df.shape)\n",
    "\n",
    "# Prepare training and validation data\n",
    "X_train, y_train = prepare_data(train_df)\n",
    "X_val, y_val = prepare_data(val_df)\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train_cat = to_categorical(y_train, num_classes=len(genres))\n",
    "y_val_cat = to_categorical(y_val, num_classes=len(genres))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train_cat.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"y_val shape:\", y_val_cat.shape)\n",
    "\n",
    "# Training details:\n",
    "# - Batch size: 64\n",
    "# - Epochs: 100 000 epochs\n",
    "# - Learning rate: 0.001 via the Adam optimizer\n",
    "# - Loss: Categorical Crossentropy\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.97,\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    mode='min',\n",
    "    min_delta=0.0001,\n",
    "    cooldown=2,\n",
    "    min_lr=1e-10\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train_cat,\n",
    "    batch_size=256,\n",
    "    epochs=100000,\n",
    "    validation_data=(X_val, y_val_cat),\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Print final training and validation accuracy\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "print(\"\\nFinal Training Accuracy: {:.2f}%\".format(final_train_acc * 100))\n",
    "print(\"Final Validation Accuracy: {:.2f}%\".format(final_val_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the save paths\n",
    "model_save_path_keras = \"./saved_models/CONEqNet.keras\"\n",
    "model_save_path_h5 = \"./saved_models/CONEqNet.h5\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(model_save_path_keras), exist_ok=True)\n",
    "\n",
    "# Save the model in TensorFlow's recommended format (.keras)\n",
    "model.save(model_save_path_keras)\n",
    "print(f\"Model saved successfully to {model_save_path_keras}\")\n",
    "\n",
    "# Save the model in HDF5 format (.h5) for compatibility with older Keras versions\n",
    "model.save(model_save_path_h5)\n",
    "print(f\"Model saved successfully to {model_save_path_h5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify a song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import librosa\n",
    "\n",
    "def extract_mfcc_from_segment(segment, sr, n_mfcc=13, hop_length=1103):\n",
    "    \"\"\"\n",
    "    Extract MFCC features from an audio segment.\n",
    "    Returns an array of shape (n_mfcc, n_frames).\n",
    "    \"\"\"\n",
    "    mfcc = librosa.feature.mfcc(y=segment, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length)\n",
    "    return mfcc\n",
    "\n",
    "def process_segment(segment, sr, target_frames=600, n_mfcc=13, hop_length=1103):\n",
    "    \"\"\"\n",
    "    Process an audio segment to extract MFCC features and prepare them for prediction.\n",
    "    Steps:\n",
    "      - Extract MFCC features.\n",
    "      - Transpose to shape (n_frames, n_mfcc).\n",
    "      - Pad or truncate to exactly target_frames.\n",
    "      - Add channel dimension, then batch dimension.\n",
    "    \"\"\"\n",
    "    mfcc = extract_mfcc_from_segment(segment, sr, n_mfcc, hop_length)\n",
    "    mfcc = mfcc.T  # now shape is (n_frames, n_mfcc)\n",
    "    \n",
    "    # Pad or truncate to ensure exactly target_frames are present\n",
    "    if mfcc.shape[0] < target_frames:\n",
    "        pad_width = target_frames - mfcc.shape[0]\n",
    "        mfcc = np.pad(mfcc, ((0, pad_width), (0, 0)), mode='constant')\n",
    "    elif mfcc.shape[0] > target_frames:\n",
    "        mfcc = mfcc[:target_frames, :]\n",
    "    \n",
    "    # Add channel dimension -> (target_frames, n_mfcc, 1)\n",
    "    mfcc = mfcc[..., np.newaxis]\n",
    "    # Add batch dimension -> (1, target_frames, n_mfcc, 1)\n",
    "    return np.expand_dims(mfcc, axis=0)\n",
    "\n",
    "# --- Load the saved model ---\n",
    "model_path = \"./saved_models/CONEqNet.keras\"\n",
    "if os.path.exists(model_path):\n",
    "    model = load_model(model_path)\n",
    "    print(f\"Loaded model from {model_path}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Model file not found. Please check the path.\")\n",
    "\n",
    "def predict_long_audio(file_path, segment_duration=30, overlap=15,\n",
    "                       target_frames=600, n_mfcc=13, hop_length=1103):\n",
    "    \"\"\"\n",
    "    Splits a long audio file into overlapping segments,\n",
    "    processes each segment to extract MFCC features,\n",
    "    and predicts the genre for each segment.\n",
    "    Finally, averages the predictions to output an overall genre.\n",
    "    \"\"\"\n",
    "    # Load the full audio file (with original sample rate)\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    segment_samples = int(segment_duration * sr)\n",
    "    overlap_samples = int(overlap * sr)\n",
    "    step_samples = segment_samples - overlap_samples\n",
    "    \n",
    "    predictions = []\n",
    "    segment_count = 0\n",
    "    \n",
    "    # Iterate through the audio with the given step size\n",
    "    for start in range(0, len(y) - segment_samples + 1, step_samples):\n",
    "        segment = y[start:start+segment_samples]\n",
    "        x_input = process_segment(segment, sr, target_frames, n_mfcc, hop_length)\n",
    "        pred = model.predict(x_input)\n",
    "        predictions.append(pred)\n",
    "        segment_count += 1\n",
    "        print(f\"Processed segment {segment_count}\")\n",
    "    \n",
    "    # Convert predictions to numpy array and average over segments\n",
    "    predictions = np.array(predictions)  # shape: (num_segments, 1, num_classes)\n",
    "    avg_prediction = np.mean(predictions, axis=0)  # shape: (1, num_classes)\n",
    "    overall_index = np.argmax(avg_prediction)\n",
    "    \n",
    "    return overall_index, avg_prediction, predictions\n",
    "\n",
    "# List of genres (used to map prediction indices)\n",
    "genres = ['blues', 'classical', 'country', 'disco', 'hiphop',\n",
    "          'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
    "\n",
    "# Provide the path to your audio file (mp3, wav, etc.)\n",
    "audio_file = \"./music/rammstein-deuschland.mp3\"  # Replace with your actual file path\n",
    "\n",
    "# Predict the overall genre for the long audio file\n",
    "overall_index, avg_prediction, segment_predictions = predict_long_audio(\n",
    "    audio_file,\n",
    "    segment_duration=30,  # duration of each segment in seconds\n",
    "    overlap=15            # overlap in seconds between segments\n",
    ")\n",
    "\n",
    "overall_genre = genres[overall_index]\n",
    "print(f\"\\nOverall Predicted Genre: {overall_genre}\")\n",
    "\n",
    "# Top 3 genres based on average prediction\n",
    "top3 = np.argsort(avg_prediction[0])[::-1][:3]\n",
    "print(\"Top 3 genres overall:\", [genres[i] for i in top3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equalize a song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "import tensorflow as tf\n",
    "import IPython.display as ipd\n",
    "from scipy import signal\n",
    "from tqdm import tqdm  # For progress bars\n",
    "\n",
    "# Step 1: Load the audio file\n",
    "def load_audio(file_path, sr=44100):\n",
    "    \"\"\"\n",
    "    Load an audio file and return the audio time series and sampling rate.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the audio file\n",
    "    sr (int): Target sampling rate\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (audio_time_series, sampling_rate)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        audio, sr = librosa.load(file_path, sr=sr)\n",
    "        print(f\"Successfully loaded audio: {len(audio)/sr:.2f} seconds at {sr} Hz\")\n",
    "        return audio, sr\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio file: {e}\")\n",
    "        # Return a short silent audio segment as fallback\n",
    "        return np.zeros(sr), sr\n",
    "\n",
    "# Step 2: Visualize the waveform and spectrogram\n",
    "def visualize_audio(audio, sr, title=\"Original Audio\"):\n",
    "    \"\"\"\n",
    "    Visualize the waveform and spectrogram of an audio signal.\n",
    "    \n",
    "    Parameters:\n",
    "    audio (numpy.ndarray): Audio time series\n",
    "    sr (int): Sampling rate\n",
    "    title (str): Title for the plot\n",
    "    \"\"\"\n",
    "    # If audio is longer than 30 seconds, only visualize the first 30 seconds\n",
    "    if len(audio) > 30 * sr:\n",
    "        print(f\"Audio is longer than 30 seconds, visualizing only the first 30 seconds\")\n",
    "        audio_to_viz = audio[:30 * sr]\n",
    "    else:\n",
    "        audio_to_viz = audio\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # Plot waveform\n",
    "    plt.subplot(2, 1, 1)\n",
    "    librosa.display.waveshow(audio_to_viz, sr=sr)\n",
    "    plt.title(f\"{title} - Waveform\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    \n",
    "    # Plot spectrogram\n",
    "    plt.subplot(2, 1, 2)\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio_to_viz)), ref=np.max)\n",
    "    librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(f\"{title} - Spectrogram\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Step 3: Create an equalization profile\n",
    "def create_eq_profile(num_bands=10, min_freq=20, max_freq=22050, sr=44100):\n",
    "    \"\"\"\n",
    "    Create an equalization profile with specified number of bands.\n",
    "    \n",
    "    Parameters:\n",
    "    num_bands (int): Number of frequency bands\n",
    "    min_freq (int): Minimum frequency (Hz)\n",
    "    max_freq (int): Maximum frequency (Hz)\n",
    "    sr (int): Sampling rate\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (center_frequencies, gains)\n",
    "    \"\"\"\n",
    "    # Ensure max_freq doesn't exceed Nyquist frequency\n",
    "    max_freq = min(max_freq, sr // 2 - 1)\n",
    "    \n",
    "    # Create logarithmically spaced center frequencies\n",
    "    center_freqs = np.logspace(np.log10(min_freq), np.log10(max_freq), num_bands)\n",
    "    \n",
    "    # Initialize gains to 0 dB (no change)\n",
    "    gains = np.zeros(num_bands)\n",
    "    \n",
    "    return center_freqs, gains\n",
    "\n",
    "# Step 4: Display and modify the equalization profile\n",
    "def set_eq_gains(center_freqs, gains):\n",
    "    \"\"\"\n",
    "    Set custom gain values for specific frequency bands.\n",
    "    \n",
    "    Parameters:\n",
    "    center_freqs (numpy.ndarray): Center frequencies\n",
    "    gains (numpy.ndarray): Gain values in dB\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Updated gain values\n",
    "    \"\"\"\n",
    "    # Display the current center frequencies and gains\n",
    "    for i, (freq, gain) in enumerate(zip(center_freqs, gains)):\n",
    "        print(f\"Band {i+1}: {freq:.1f} Hz = {gain:.1f} dB\")\n",
    "    \n",
    "    # Example: Update gains for specific bands (modify as needed)\n",
    "    return gains\n",
    "\n",
    "# Step 5: Apply equalization using FFT-based approach\n",
    "def apply_eq_fft(audio, sr, center_freqs, gains):\n",
    "    \"\"\"\n",
    "    Apply equalization using FFT-based method.\n",
    "    \n",
    "    Parameters:\n",
    "    audio (numpy.ndarray): Audio time series\n",
    "    sr (int): Sampling rate\n",
    "    center_freqs (numpy.ndarray): Center frequencies\n",
    "    gains (numpy.ndarray): Gain values in dB\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Equalized audio\n",
    "    \"\"\"\n",
    "    n_fft = 2048  # Next power of 2 for efficient FFT\n",
    "    stft = librosa.stft(audio, n_fft=n_fft)\n",
    "    magnitude, phase = librosa.magphase(stft)\n",
    "    freq_bins = librosa.fft_frequencies(sr=sr, n_fft=n_fft)\n",
    "    \n",
    "    # Create an equalization curve (default no change)\n",
    "    eq_curve = np.ones_like(freq_bins)\n",
    "    \n",
    "    for center_freq, gain_db in zip(center_freqs, gains):\n",
    "        gain_linear = 10 ** (gain_db / 20)\n",
    "        lower_freq = center_freq / np.sqrt(2)\n",
    "        upper_freq = center_freq * np.sqrt(2)\n",
    "        band_indices = np.where((freq_bins >= lower_freq) & (freq_bins <= upper_freq))\n",
    "        eq_curve[band_indices] *= gain_linear\n",
    "    \n",
    "    eq_magnitude = magnitude * eq_curve[:, np.newaxis]\n",
    "    eq_stft = eq_magnitude * phase\n",
    "    eq_audio = librosa.istft(eq_stft, length=len(audio))\n",
    "    return eq_audio\n",
    "\n",
    "# Step 6: Process audio in segments\n",
    "def process_audio_in_segments(audio, sr, center_freqs, gains, segment_duration=30, overlap=3):\n",
    "    \"\"\"\n",
    "    Process audio in segments with overlap to avoid artifacts at boundaries.\n",
    "    \n",
    "    Parameters:\n",
    "    audio (numpy.ndarray): Audio time series\n",
    "    sr (int): Sampling rate\n",
    "    center_freqs (numpy.ndarray): Center frequencies for EQ\n",
    "    gains (numpy.ndarray): Gain values to apply (same for all segments)\n",
    "    segment_duration (int): Duration of each segment in seconds\n",
    "    overlap (int): Overlap between segments in seconds\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Processed audio\n",
    "    \"\"\"\n",
    "    segment_length = int(segment_duration * sr)\n",
    "    overlap_length = int(overlap * sr)\n",
    "    hop_length = segment_length - overlap_length\n",
    "    total_samples = len(audio)\n",
    "    num_segments = int(np.ceil((total_samples - overlap_length) / hop_length)) + 1\n",
    "    \n",
    "    processed_audio = np.zeros_like(audio)\n",
    "    normalization = np.zeros_like(audio)\n",
    "    \n",
    "    fade_len = overlap_length\n",
    "    fade_in = np.linspace(0, 1, fade_len)\n",
    "    fade_out = np.linspace(1, 0, fade_len)\n",
    "    \n",
    "    print(f\"Processing audio in {num_segments} segments...\")\n",
    "    for i in tqdm(range(num_segments)):\n",
    "        start_idx = i * hop_length\n",
    "        end_idx = min(start_idx + segment_length, total_samples)\n",
    "        if start_idx >= total_samples:\n",
    "            break\n",
    "        segment = audio[start_idx:end_idx]\n",
    "        processed_segment = apply_eq_fft(segment, sr, center_freqs, gains)\n",
    "        \n",
    "        window = np.ones(len(segment))\n",
    "        if i > 0:\n",
    "            window[:fade_len] = fade_in\n",
    "        if i < num_segments - 1 and len(segment) >= fade_len:\n",
    "            window[-fade_len:] = fade_out\n",
    "        \n",
    "        processed_segment *= window\n",
    "        processed_audio[start_idx:end_idx] += processed_segment\n",
    "        normalization[start_idx:end_idx] += window\n",
    "    \n",
    "    mask = normalization > 0\n",
    "    processed_audio[mask] /= normalization[mask]\n",
    "    return processed_audio\n",
    "\n",
    "# Step 7: Play and save audio\n",
    "def play_audio(audio, sr, title=\"Audio\"):\n",
    "    \"\"\"\n",
    "    Play audio in the Jupyter notebook.\n",
    "    \n",
    "    Parameters:\n",
    "    audio (numpy.ndarray): Audio time series\n",
    "    sr (int): Sampling rate\n",
    "    title (str): Title for the audio player\n",
    "    \"\"\"\n",
    "    print(f\"Playing {title}\")\n",
    "    return ipd.Audio(audio, rate=sr)\n",
    "\n",
    "def save_audio(audio, sr, file_path):\n",
    "    \"\"\"\n",
    "    Save audio to file.\n",
    "    \n",
    "    Parameters:\n",
    "    audio (numpy.ndarray): Audio time series\n",
    "    sr (int): Sampling rate\n",
    "    file_path (str): Output file path\n",
    "    \"\"\"\n",
    "    max_amplitude = np.max(np.abs(audio))\n",
    "    normalized_audio = audio / max_amplitude * 0.95 if max_amplitude > 0 else audio\n",
    "    sf.write(file_path, normalized_audio, sr)\n",
    "    print(f\"Saved equalized audio to {file_path}\")\n",
    "\n",
    "# Step 8: Define some preset equalization profiles\n",
    "def apply_eq_preset(preset_name):\n",
    "    \"\"\"\n",
    "    Return gain values for common EQ presets.\n",
    "    \n",
    "    Parameters:\n",
    "    preset_name (str): Name of the preset ('bass_boost', 'vocal_enhance', etc.)\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Gain values for 10-band EQ\n",
    "    \"\"\"\n",
    "    presets = {\n",
    "        'flat': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        'bass_boost': [7, 5, 3, 1, 0, 0, 0, 0, 0, 0],\n",
    "        'bass_cut': [-7, -5, -3, -1, 0, 0, 0, 0, 0, 0],\n",
    "        'treble_boost': [0, 0, 0, 0, 0, 0, 1, 3, 5, 7],\n",
    "        'treble_cut': [0, 0, 0, 0, 0, 0, -1, -3, -5, -7],\n",
    "        'vocal_enhance': [-2, -2, 0, 2, 4, 3, 2, 1, 0, -1],\n",
    "        'v_shape': [4, 2, 0, -2, -3, -3, -2, 0, 2, 4],\n",
    "        'loudness': [5, 4, 2, 0, -1, -1, 0, 2, 4, 5],\n",
    "        'crazy': [20, 15, 10, 5, 2, 1, 0, 0, 0, 0]\n",
    "    }\n",
    "    \n",
    "    if preset_name in presets:\n",
    "        return np.array(presets[preset_name])\n",
    "    else:\n",
    "        print(f\"Preset '{preset_name}' not found. Available presets: {list(presets.keys())}\")\n",
    "        return np.zeros(10)\n",
    "try:\n",
    "    # Step 1: Load or generate audio\n",
    "    file_path = \"./music/wtc-cream.mp3\"  # Change to your file path\n",
    "    try:\n",
    "        audio, sr = load_audio(file_path)\n",
    "    except:\n",
    "        print(f\"Couldn't load {file_path}\")\n",
    "    \n",
    "    # Step 2: Visualize original audio (first 30 seconds)\n",
    "    visualize_audio(audio, sr, \"Original Audio\")\n",
    "    \n",
    "    # Step 3: Create equalization profile\n",
    "    center_freqs, gains = create_eq_profile(num_bands=10, sr=sr)\n",
    "    \n",
    "    # Step 4: Apply an EQ preset (change preset name as desired)\n",
    "    gains = apply_eq_preset('crazy')\n",
    "    print(\"Using EQ preset 'bass_boost' with gains:\", gains)\n",
    "    \n",
    "    # Step 5: Process audio in segments (30s segments with 3s overlap)\n",
    "    processed_audio = process_audio_in_segments(audio, sr, center_freqs, gains, segment_duration=30, overlap=3)\n",
    "    \n",
    "    # Step 6: Visualize the equalized audio\n",
    "    visualize_audio(processed_audio, sr, \"Equalized Audio\")\n",
    "    \n",
    "    # Step 7: Play original and equalized audio\n",
    "    print(\"Playing Original Audio:\")\n",
    "    ipd.display(play_audio(audio, sr, \"Original Audio\"))\n",
    "    print(\"Playing Equalized Audio:\")\n",
    "    ipd.display(play_audio(processed_audio, sr, \"Equalized Audio\"))\n",
    "    \n",
    "    # Step 8: Save the equalized audio to file\n",
    "    output_path = \"./music/wtc-cream_equalized.mp3\"\n",
    "    save_audio(processed_audio, sr, output_path)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred in the processing pipeline:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EQ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "import tensorflow as tf\n",
    "import IPython.display as ipd\n",
    "from scipy import signal\n",
    "from tqdm import tqdm  # For progress bars\n",
    "\n",
    "# Step 1: Load the audio file\n",
    "def load_audio(file_path, sr=44100):\n",
    "    \"\"\"\n",
    "    Load an audio file and return the audio time series and sampling rate.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the audio file\n",
    "    sr (int): Target sampling rate\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (audio_time_series, sampling_rate)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        audio, sr = librosa.load(file_path, sr=sr)\n",
    "        print(f\"Successfully loaded audio: {len(audio)/sr:.2f} seconds at {sr} Hz\")\n",
    "        return audio, sr\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio file: {e}\")\n",
    "        # Return a short silent audio segment as fallback\n",
    "        return np.zeros(sr), sr\n",
    "    \n",
    "def extract_mfcc_from_segment(segment, sr, n_mfcc=13, hop_length=1103):\n",
    "    \"\"\"\n",
    "    Extract MFCC features from an audio segment.\n",
    "    Returns an array of shape (n_mfcc, n_frames).\n",
    "    \"\"\"\n",
    "    mfcc = librosa.feature.mfcc(y=segment, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length)\n",
    "    return mfcc\n",
    "\n",
    "# Step 2: Visualize the waveform and spectrogram\n",
    "def visualize_audio(audio, sr, title=\"Original Audio\"):\n",
    "    \"\"\"\n",
    "    Visualize the waveform and spectrogram of an audio signal.\n",
    "    \n",
    "    Parameters:\n",
    "    audio (numpy.ndarray): Audio time series\n",
    "    sr (int): Sampling rate\n",
    "    title (str): Title for the plot\n",
    "    \"\"\"\n",
    "    # If audio is longer than 30 seconds, only visualize the first 30 seconds\n",
    "    if len(audio) > 30 * sr:\n",
    "        print(f\"Audio is longer than 30 seconds, visualizing only the first 30 seconds\")\n",
    "        audio_to_viz = audio[:30 * sr]\n",
    "    else:\n",
    "        audio_to_viz = audio\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # Plot waveform\n",
    "    plt.subplot(2, 1, 1)\n",
    "    librosa.display.waveshow(audio_to_viz, sr=sr)\n",
    "    plt.title(f\"{title} - Waveform\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    \n",
    "    # Plot spectrogram\n",
    "    plt.subplot(2, 1, 2)\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio_to_viz)), ref=np.max)\n",
    "    librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(f\"{title} - Spectrogram\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Step 3: Create an equalization profile\n",
    "def create_eq_profile(num_bands=10, min_freq=20, max_freq=22050, sr=44100):\n",
    "    \"\"\"\n",
    "    Create an equalization profile with specified number of bands.\n",
    "    \n",
    "    Parameters:\n",
    "    num_bands (int): Number of frequency bands\n",
    "    min_freq (int): Minimum frequency (Hz)\n",
    "    max_freq (int): Maximum frequency (Hz)\n",
    "    sr (int): Sampling rate\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (center_frequencies, gains)\n",
    "    \"\"\"\n",
    "    # Ensure max_freq doesn't exceed Nyquist frequency\n",
    "    max_freq = min(max_freq, sr // 2 - 1)\n",
    "    \n",
    "    # Create logarithmically spaced center frequencies\n",
    "    center_freqs = np.logspace(np.log10(min_freq), np.log10(max_freq), num_bands)\n",
    "    \n",
    "    # Initialize gains to 0 dB (no change)\n",
    "    gains = np.zeros(num_bands)\n",
    "    \n",
    "    return center_freqs, gains\n",
    "\n",
    "# Step 4: Display and modify the equalization profile\n",
    "def set_eq_gains(center_freqs, gains):\n",
    "    \"\"\"\n",
    "    Set custom gain values for specific frequency bands.\n",
    "    \n",
    "    Parameters:\n",
    "    center_freqs (numpy.ndarray): Center frequencies\n",
    "    gains (numpy.ndarray): Gain values in dB\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Updated gain values\n",
    "    \"\"\"\n",
    "    # Display the current center frequencies and gains\n",
    "    for i, (freq, gain) in enumerate(zip(center_freqs, gains)):\n",
    "        print(f\"Band {i+1}: {freq:.1f} Hz = {gain:.1f} dB\")\n",
    "    \n",
    "    # Example: Update gains for specific bands (modify as needed)\n",
    "    return gains\n",
    "\n",
    "# Step 5: Apply equalization using FFT-based approach\n",
    "def apply_eq_fft(audio, sr, center_freqs, gains):\n",
    "    \"\"\"\n",
    "    Apply equalization using FFT-based method.\n",
    "    \n",
    "    Parameters:\n",
    "    audio (numpy.ndarray): Audio time series\n",
    "    sr (int): Sampling rate\n",
    "    center_freqs (numpy.ndarray): Center frequencies\n",
    "    gains (numpy.ndarray): Gain values in dB\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Equalized audio\n",
    "    \"\"\"\n",
    "    n_fft = 2048  # Next power of 2 for efficient FFT\n",
    "    stft = librosa.stft(audio, n_fft=n_fft)\n",
    "    magnitude, phase = librosa.magphase(stft)\n",
    "    freq_bins = librosa.fft_frequencies(sr=sr, n_fft=n_fft)\n",
    "    \n",
    "    # Create an equalization curve (default no change)\n",
    "    eq_curve = np.ones_like(freq_bins)\n",
    "    \n",
    "    for center_freq, gain_db in zip(center_freqs, gains):\n",
    "        gain_linear = 10 ** (gain_db / 20)\n",
    "        lower_freq = center_freq / np.sqrt(2)\n",
    "        upper_freq = center_freq * np.sqrt(2)\n",
    "        band_indices = np.where((freq_bins >= lower_freq) & (freq_bins <= upper_freq))\n",
    "        eq_curve[band_indices] *= gain_linear\n",
    "    \n",
    "    eq_magnitude = magnitude * eq_curve[:, np.newaxis]\n",
    "    eq_stft = eq_magnitude * phase\n",
    "    eq_audio = librosa.istft(eq_stft, length=len(audio))\n",
    "    return eq_audio\n",
    "\n",
    "# Step 6: Process audio in segments\n",
    "def process_audio_in_segments(audio, sr, center_freqs, segment_duration=30, overlap=3, intensity=1.0):\n",
    "    \"\"\"\n",
    "    Process audio in segments with overlap to avoid artifacts at boundaries.\n",
    "    \n",
    "    Parameters:\n",
    "    audio (numpy.ndarray): Audio time series\n",
    "    sr (int): Sampling rate\n",
    "    center_freqs (numpy.ndarray): Center frequencies for EQ\n",
    "    gains (numpy.ndarray): Gain values to apply (same for all segments)\n",
    "    segment_duration (int): Duration of each segment in seconds\n",
    "    overlap (int): Overlap between segments in seconds\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Processed audio\n",
    "    \"\"\"\n",
    "    segment_length = int(segment_duration * sr)\n",
    "    overlap_length = int(overlap * sr)\n",
    "    hop_length = segment_length - overlap_length\n",
    "    total_samples = len(audio)\n",
    "    num_segments = int(np.ceil((total_samples - overlap_length) / hop_length)) + 1\n",
    "    \n",
    "    processed_audio = np.zeros_like(audio)\n",
    "    normalization = np.zeros_like(audio)\n",
    "    \n",
    "    fade_len = overlap_length\n",
    "    fade_in = np.linspace(0, 1, fade_len)\n",
    "    fade_out = np.linspace(1, 0, fade_len)\n",
    "\n",
    "    genres = ['blues', 'classical', 'country', 'disco', 'hiphop',\n",
    "                 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
    "    \n",
    "    print(f\"Processing audio in {num_segments} segments...\")\n",
    "    for i in tqdm(range(num_segments)):\n",
    "        start_idx = i * hop_length\n",
    "        end_idx = min(start_idx + segment_length, total_samples)\n",
    "        if start_idx >= total_samples:\n",
    "            break\n",
    "\n",
    "        segment = audio[start_idx:end_idx]\n",
    "        x_input = process_segment(segment, sr, target_frames=600, n_mfcc=13, hop_length=1103)\n",
    "        pred = model.predict(x_input)\n",
    "        genre_index = np.argmax(pred)\n",
    "        genre_preset = genres[genre_index]\n",
    "        print(f\"Predicted genre for segment {i+1}: {genre_preset}\")\n",
    "\n",
    "        gains = apply_eq_preset(genre_preset, intensity=intensity)\n",
    "        processed_segment = apply_eq_fft(segment, sr, center_freqs, gains)\n",
    "        \n",
    "        window = np.ones(len(segment))\n",
    "        if i > 0:\n",
    "            window[:fade_len] = fade_in\n",
    "        if i < num_segments - 1 and len(segment) >= fade_len:\n",
    "            window[-fade_len:] = fade_out\n",
    "        \n",
    "        processed_segment *= window\n",
    "        processed_audio[start_idx:end_idx] += processed_segment\n",
    "        normalization[start_idx:end_idx] += window\n",
    "    \n",
    "    mask = normalization > 0\n",
    "    processed_audio[mask] /= normalization[mask]\n",
    "    return processed_audio\n",
    "\n",
    "def process_segment(segment, sr, target_frames=600, n_mfcc=13, hop_length=1103):\n",
    "    \"\"\"\n",
    "    Process an audio segment to extract MFCC features and prepare them for prediction.\n",
    "    Steps:\n",
    "      - Extract MFCC features.\n",
    "      - Transpose to shape (n_frames, n_mfcc).\n",
    "      - Pad or truncate to exactly target_frames.\n",
    "      - Add channel dimension, then batch dimension.\n",
    "    \"\"\"\n",
    "    mfcc = extract_mfcc_from_segment(segment, sr, n_mfcc, hop_length)\n",
    "    mfcc = mfcc.T  # now shape is (n_frames, n_mfcc)\n",
    "    \n",
    "    # Pad or truncate to ensure exactly target_frames are present\n",
    "    if mfcc.shape[0] < target_frames:\n",
    "        pad_width = target_frames - mfcc.shape[0]\n",
    "        mfcc = np.pad(mfcc, ((0, pad_width), (0, 0)), mode='constant')\n",
    "    elif mfcc.shape[0] > target_frames:\n",
    "        mfcc = mfcc[:target_frames, :]\n",
    "    \n",
    "    # Add channel dimension -> (target_frames, n_mfcc, 1)\n",
    "    mfcc = mfcc[..., np.newaxis]\n",
    "    # Add batch dimension -> (1, target_frames, n_mfcc, 1)\n",
    "    return np.expand_dims(mfcc, axis=0)\n",
    "\n",
    "# Step 7: Play and save audio\n",
    "def play_audio(audio, sr, title=\"Audio\"):\n",
    "    \"\"\"\n",
    "    Play audio in the Jupyter notebook.\n",
    "    \n",
    "    Parameters:\n",
    "    audio (numpy.ndarray): Audio time series\n",
    "    sr (int): Sampling rate\n",
    "    title (str): Title for the audio player\n",
    "    \"\"\"\n",
    "    print(f\"Playing {title}\")\n",
    "    return ipd.Audio(audio, rate=sr)\n",
    "\n",
    "def save_audio(audio, sr, file_path):\n",
    "    \"\"\"\n",
    "    Save audio to file.\n",
    "    \n",
    "    Parameters:\n",
    "    audio (numpy.ndarray): Audio time series\n",
    "    sr (int): Sampling rate\n",
    "    file_path (str): Output file path\n",
    "    \"\"\"\n",
    "    max_amplitude = np.max(np.abs(audio))\n",
    "    normalized_audio = audio / max_amplitude * 0.95 if max_amplitude > 0 else audio\n",
    "    sf.write(file_path, normalized_audio, sr)\n",
    "    print(f\"Saved equalized audio to {file_path}\")\n",
    "\n",
    "# Step 8: Define some preset equalization profiles\n",
    "def apply_eq_preset(preset_name, intensity=1.0):\n",
    "    \"\"\"\n",
    "    Return gain values for common EQ presets.\n",
    "    \n",
    "    Parameters:\n",
    "    preset_name (str): Name of the preset ('bass_boost', 'vocal_enhance', etc.)\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Gain values for 10-band EQ\n",
    "    \"\"\"\n",
    "    genre_presets = {\n",
    "    'blues':     [-1,  0,  2,  2,  1,  0, -1,  0,  1,  1],\n",
    "    'classical': [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "    'country':   [ 0,  1,  1,  2,  1,  0,  0,  0,  0,  0],\n",
    "    'disco':     [ 2,  3,  2,  1,  0,  0,  1,  2,  3,  2],\n",
    "    'hiphop':    [ 3,  4,  2,  0, -1, -1,  0,  1,  1,  0],\n",
    "    'jazz':      [ 0,  1,  1,  2,  2,  1,  0,  0,  1,  0],\n",
    "    'metal':     [ 2,  3,  0, -3, -4, -3,  0,  3,  3,  2],\n",
    "    'pop':       [ 0,  1,  2,  2,  1,  0,  1,  1,  2,  2],\n",
    "    'reggae':    [ 0,  1,  0,  0, -1, -1,  0,  0,  1,  0],\n",
    "    'rock':      [ 1,  1,  0,  0,  1,  1,  0,  0,  1,  1]\n",
    "    }\n",
    "    \n",
    "    if preset_name in genre_presets:\n",
    "        return np.array(genre_presets[preset_name]) * intensity\n",
    "    else:\n",
    "        print(f\"Preset '{preset_name}' not found. Available presets: {list(genre_presets.keys())}\")\n",
    "        return np.zeros(10)\n",
    "    \n",
    "# --- Load the saved model ---\n",
    "model_path = \"./saved_models/CONEqNet.keras\"\n",
    "if os.path.exists(model_path):\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    print(f\"Loaded model from {model_path}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Model file not found. Please check the path.\")\n",
    "\n",
    "try:\n",
    "    # Step 1: Load or generate audio\n",
    "    dir_path = \"./music/\"\n",
    "    input_file_name = \"rammstein-deuschland.mp3\"\n",
    "    output_file_name = input_file_name.replace(\".mp3\", \"-equalized-ai.mp3\")\n",
    "    file_path = os.path.join(dir_path, input_file_name)\n",
    "    output_path = os.path.join(dir_path, output_file_name)\n",
    "\n",
    "    intensity = 1.0\n",
    "\n",
    "    try:\n",
    "        audio, sr = load_audio(file_path)\n",
    "    except:\n",
    "        print(f\"Couldn't load {file_path}\")\n",
    "    \n",
    "    # Step 2: Visualize original audio (first 30 seconds)\n",
    "    visualize_audio(audio, sr, \"Original Audio\")\n",
    "    \n",
    "    # Step 3: Create equalization profile\n",
    "    center_freqs, gains = create_eq_profile(num_bands=10, sr=sr)\n",
    "    \n",
    "    # Step 4: Process audio in segments (30s segments with 3s overlap)\n",
    "    processed_audio = process_audio_in_segments(audio, sr, center_freqs, segment_duration=30, overlap=3, intensity=intensity)\n",
    "    \n",
    "    # Step 5: Visualize the equalized audio\n",
    "    visualize_audio(processed_audio, sr, \"Equalized Audio\")\n",
    "    \n",
    "    # Step 6: Play original and equalized audio\n",
    "    print(\"Playing Original Audio:\")\n",
    "    ipd.display(play_audio(audio, sr, \"Original Audio\"))\n",
    "    print(\"Playing Equalized Audio:\")\n",
    "    ipd.display(play_audio(processed_audio, sr, \"Equalized Audio\"))\n",
    "    \n",
    "    # Step 7: Save the equalized audio to file\n",
    "    save_audio(processed_audio, sr, output_path)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred in the processing pipeline:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pred 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "# Load the pre-trained PANNs CNN14 model from Torch Hub.\n",
    "# This model is trained on AudioSet (527 classes)\n",
    "model = torch.hub.load('qiuqiangkong/audioset_tagging_cnn', 'cnn14', pretrained=True, force_reload=True)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Define a function to preprocess audio and run prediction\n",
    "def predict_genre(audio_path, target_sr=32000):\n",
    "    \"\"\"\n",
    "    Load an audio file, resample as needed, and run the pre-trained CNN14 model\n",
    "    to obtain clip-level predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    audio_path (str): Path to the audio file (can be mp3, wav, etc.)\n",
    "    target_sr (int): Sampling rate expected by the model (default 32000 Hz)\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray: The predicted output (clipwise_output) from CNN14.\n",
    "    \"\"\"\n",
    "    # Load audio using torchaudio (returns tensor shape [channels, samples])\n",
    "    waveform, sr = torchaudio.load(audio_path)\n",
    "    \n",
    "    # If the sampling rate is different from target_sr, resample the waveform.\n",
    "    if sr != target_sr:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sr)\n",
    "        waveform = resampler(waveform)\n",
    "    \n",
    "    # Ensure mono: average if more than one channel.\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    \n",
    "    # Add batch dimension: model expects (batch, channels, samples)\n",
    "    waveform = waveform.unsqueeze(0)\n",
    "    \n",
    "    # Run prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(waveform)\n",
    "    \n",
    "    # The model returns a dict; 'clipwise_output' is a tensor of shape [batch, num_classes]\n",
    "    predictions = output['clipwise_output'].squeeze(0).cpu().numpy()\n",
    "    return predictions\n",
    "\n",
    "# Example usage:\n",
    "audio_path = \"your_audio_file.mp3\"  # Replace with your actual audio file path\n",
    "preds = predict_genre(audio_path)\n",
    "print(\"Model predictions:\", preds)\n",
    "\n",
    "# Mapping to a smaller set of music genres:\n",
    "# The output 'preds' is a probability distribution over 527 classes.\n",
    "# You may need to create a mapping or filter out predictions corresponding to music genres.\n",
    "# For example:\n",
    "# genre_mapping = {'rock': [...], 'pop': [...], ...}\n",
    "# and then aggregate the scores from the relevant indices.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AI)",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
